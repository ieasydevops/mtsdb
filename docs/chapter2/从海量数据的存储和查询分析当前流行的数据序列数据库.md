# 2.5 从海量数据的存储和查询分析当前流行的时间序列数据库

## 2.5.1 OpenTSDB

### 海量数据的存储

OpenTSDB自身并不实现分布式存储，而是借助于HBase或者Cassandra来存储海量的数据。

### 查询分析

HBase自身目前暂时不支持二级索引或者倒排索引

## 2.5.2 Influxdb

### 海量数据的存储

通过hash分片的方式来存储海量数据,通过LSM方式来实现数据的快速写入

### 查询分析

* 倒排索引

InfluxDB实现了，因此可以实现快速的数据过滤功能。倒排索引的代价就是降低了写入速度，再加上倒排索引的占用量可能会很大，并不能完全放内存，为了解决上述问题，倒排索引的写入也引入了LSM模型，即InfluxDB的TSI.
倒排索引针对数据过滤场景非常有优势，但是对于没有数据过滤的场景如果仍然沿用倒排索引的查询方式通过一系列series id去随机查找数据会很慢很慢

* 预聚合

InfluxDB引入了Continuous Query和Retention Policy。2者配合使用可以提高数据的查询速度.
相比于OpenTSDB中的Rollup和Pre-Aggregates,InfluxDB全部融合到了Continuous Query，既可以对时间维度进行Rollup，又可以将多个Series进行Pre-Aggregates。
其实现原理很简单：就是查询一遍聚合后的数据写入到新的指标名下或者新的Retention Policy下. 值得注意的是：** 每次查询都是查询最近一段时间范围的数据，对于之前已经查询过的时间范围若来了新的数据，并不会再次查询一次更新下结果。**

这种方式对用户来说干预很大，用户需要理解Continuous Query，并且根据自己的查询需求来编写对应的Continuous Query，在查询时又要手动去选择合适的Retention Policy去进行查询，即还不能够做到自动化，InfluxDB自己又不能对所有metric都自动执行Continuous Query, 因为它不知道该如何聚合，不同用户写入的metric指标可能有不同的聚合需求。

* 列式存储，借鉴了Facebook的gorilla 的压缩算法

* 支持时间范围的高效过滤

## 2.5.3 Druid


## 2.5.4 Elasticsearch



## 2.5.6 Kylin


## 2.5.7 LinDB

### 2.5.7.1 海量数据如何存储

借鉴Kafka的集群功能来实现海量数据的存储，目前只依赖ZooKeeper，并且在部署方面完全可以任意台部署，并不要求至少3台

LinDB内部也是采用LSM方式来实现快速写入

### 2.5.7.2 查询分析

#### 倒排索引

* InfluxDB:基本实现方式是tagKey-tagValue-[seriesKey offset list]，作为全局索引，优点：在时序场景下，相对文件级别索引，大部分时间每个文件索引基本上都差不多的，所以在查询上只需要1次索引查询即可，不像文件级别索引每个文件都要进行索引查询。
* Druid：基本实现方式是tagKey-tagValaue-[row id list]，每个文件包含自己的索引。缺点：每个文件都要进行索引查询。优点：在数据迁移方面非常有利，只需要将整个文件复制即可，而InfluxDB就相对麻烦很多。
* LinDB:基本实现方式是tagKey-tagValue-[series Id list],也是作为全局索引，优缺点和InfluxDB一样的。LinDB这样设计主要还是基于时序场景下大部分情况下文件索引都是重复的这一情况考虑的。

#### 倒排索引大小

这里指的是上述一个list的大小

* InfluxDB、LinDB:倒排索引大小就是组合数

* Druid：倒排索引大小就是行数，在时序数据场景下，一个文件中的数据基本是所有组合数多个时间点的数据，数据行数相比组合数大了很多，因此相对InfluxDB和LinDB大了很多




## 参考文献

[数据分析之时序数据库](https://zhuanlan.zhihu.com/p/36804890)

[Documentation for OpenTSDB](http://opentsdb.net/docs/build/html/index.html)

[](http://hbasefly.com/category/%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/)